#!/usr/bin/env python3
"""
MACARON v2.1 - Massive Automated Comprehensive Asset Reconnaissance & Offensive Nexus
A configurable security reconnaissance CLI with YAML-based pipeline configuration
"""
import argparse
import sys
import json
import os
import shutil
import subprocess
import tempfile
import signal
import re
import time
import threading
from pathlib import Path
from datetime import datetime, timezone
from typing import List, Dict, Any, Optional, Set, Tuple
from dataclasses import dataclass, field
from enum import Enum
from concurrent.futures import ThreadPoolExecutor, as_completed
import logging

# YAML support
try:
    import yaml
    YAML_AVAILABLE = True
except ImportError:
    YAML_AVAILABLE = False

# Rich for beautiful CLI UI
try:
    from rich.console import Console
    from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn, TaskProgressColumn, TimeElapsedColumn, TimeRemainingColumn
    from rich.panel import Panel
    from rich.table import Table
    from rich.live import Live
    from rich.layout import Layout
    from rich.text import Text
    from rich.style import Style
    from rich import box
    from rich.tree import Tree
    from rich.syntax import Syntax
    RICH_AVAILABLE = True
except ImportError:
    RICH_AVAILABLE = False
    print("[!] Rich library not found. Install with: pip install rich")

# ============== CONSTANTS ==============

VERSION = "2.1.1"
BANNER = """[bold cyan]
â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ•—
â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ•‘
â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ•— â–ˆâ–ˆâ•‘
â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘
â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘ â•šâ–ˆâ–ˆâ–ˆâ–ˆâ•‘
â•šâ•â•     â•šâ•â•â•šâ•â•  â•šâ•â• â•šâ•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•šâ•â•  â•šâ•â• â•šâ•â•â•â•â•â• â•šâ•â•  â•šâ•â•â•â•[/]
[dim]        v{version} | github.com/root-Manas/macaron[/]
"""

# Directories
HOME_DIR = Path.home() / ".macaron"
DATA_DIR = HOME_DIR / "data"
CONFIG_DIR = HOME_DIR / "config"
STATE_DIR = HOME_DIR / "state"
LOGS_DIR = HOME_DIR / "logs"
WORDLISTS_DIR = HOME_DIR / "wordlists"
PIPELINE_FILE = CONFIG_DIR / "pipeline.yaml"

# Bundled pipeline config path (in repo)
BUNDLED_PIPELINE = Path(__file__).parent / "config" / "pipeline.yaml"

# Console for Rich output
console = Console() if RICH_AVAILABLE else None

# ============== PIPELINE CONFIG ==============

def get_default_pipeline_config() -> Dict:
    """Return default embedded pipeline config"""
    return {
        "global": {
            "rate_limit": 100,
            "threads": 25,
            "use_proxy": True,
            "slow_mode": {"rate_limit": 10, "threads": 5}
        },
        "tools": {
            "subfinder": {"cmd": "subfinder -d {target} -silent -all -t {threads}", "timeout": 600, "input": "target", "output": "stdout_lines", "installed_check": "subfinder"},
            "amass": {"cmd": "amass enum -passive -d {target} -dns-qps 50 -timeout 10", "timeout": 600, "input": "target", "output": "stdout_lines", "installed_check": "amass"},
            "assetfinder": {"cmd": "assetfinder --subs-only {target}", "timeout": 300, "input": "target", "output": "stdout_lines", "installed_check": "assetfinder"},
            "findomain": {"cmd": "findomain -t {target} -q", "timeout": 300, "input": "target", "output": "stdout_lines", "installed_check": "findomain"},
            "crtsh": {"cmd": "curl -s 'https://crt.sh/?q=%25.{target}&output=json' | jq -r '.[].name_value' | sort -u", "timeout": 60, "input": "target", "output": "stdout_lines", "installed_check": "curl"},
            "dnsx": {"cmd": "dnsx -l {input_file} -a -resp -json -silent -t {threads}", "timeout": 600, "input": "file", "output": "json_lines", "installed_check": "dnsx", "parser": "dnsx"},
            "naabu": {"cmd": "naabu -l {input_file} -json -silent -top-ports 1000 -rate {rate} -c {threads} -retries 2", "timeout": 1800, "input": "file", "output": "json_lines", "installed_check": "naabu", "parser": "naabu"},
            "naabu_quick": {"cmd": "naabu -l {input_file} -json -silent -p 80,443,8080,8443,8000,3000,5000,9000 -rate {rate}", "timeout": 300, "input": "file", "output": "json_lines", "installed_check": "naabu", "parser": "naabu"},
            "httpx": {"cmd": "httpx -l {input_file} -json -silent -sc -title -td -cdn -follow-redirects -rate-limit {rate} -threads {threads}", "timeout": 1800, "input": "file", "output": "json_lines", "installed_check": "httpx", "parser": "httpx"},
            "gau": {"cmd": "gau --subs --threads 5 {target}", "timeout": 300, "input": "target", "output": "stdout_lines", "installed_check": "gau"},
            "waybackurls": {"cmd": "echo {target} | waybackurls", "timeout": 300, "input": "target", "output": "stdout_lines", "installed_check": "waybackurls"},
            "katana": {"cmd": "katana -list {input_file} -silent -d 3 -jc -iqp -c {threads} -rl {rate}", "timeout": 900, "input": "file", "output": "stdout_lines", "installed_check": "katana"},
            "katana_deep": {"cmd": "katana -list {input_file} -silent -d 5 -jc -iqp -c {threads} -rl {rate}", "timeout": 1200, "input": "file", "output": "stdout_lines", "installed_check": "katana"},
            "hakrawler": {"cmd": "echo {target} | hakrawler -d 2 -subs -u", "timeout": 300, "input": "target", "output": "stdout_lines", "installed_check": "hakrawler"},
            "getjs": {"cmd": "getJS --url {target} --complete", "timeout": 120, "input": "target", "output": "stdout_lines", "installed_check": "getJS"},
            "ffuf": {"cmd": "ffuf -u {target}/FUZZ -w {wordlist} -o {output_file} -of json -mc 200,201,204,301,302,307,401,403,405 -t {threads} -rate {rate} -s", "timeout": 600, "input": "target", "output": "file", "installed_check": "ffuf", "options": {"wordlist": "~/.macaron/wordlists/common.txt"}},
            "gowitness": {"cmd": "gowitness scan file -f {input_file} --screenshot-path {output_dir} --delay 2 --threads {threads}", "timeout": 3600, "input": "file", "output": "directory", "installed_check": "gowitness"},
            "nuclei": {"cmd": "nuclei -l {input_file} -o {output_file} -jsonl -silent -severity {severity} -rl {rate} -c {threads} -nh", "timeout": 7200, "input": "file", "output": "json_lines", "installed_check": "nuclei", "options": {"severity": "critical,high,medium"}},
            "nuclei_fast": {"cmd": "nuclei -l {input_file} -o {output_file} -jsonl -silent -severity critical,high -rl {rate} -c {threads} -nh", "timeout": 3600, "input": "file", "output": "json_lines", "installed_check": "nuclei"},
        },
        "pipelines": {
            "wide": {
                "description": "Full infrastructure reconnaissance",
                "stages": [
                    {"name": "Subdomain Discovery", "emoji": "ðŸ”", "tools": ["subfinder", "assetfinder", "findomain", "crtsh", "amass"], "input_from": "target", "output_to": "subdomains", "enabled": True},
                    {"name": "DNS Resolution", "emoji": "ðŸ“¡", "tools": ["dnsx"], "input_from": "subdomains", "output_to": "resolved", "enabled": True},
                    {"name": "Port Scanning", "emoji": "ðŸ”Œ", "tools": ["naabu"], "input_from": "resolved", "output_to": "ports", "enabled": True},
                    {"name": "HTTP Probing", "emoji": "ðŸŒ", "tools": ["httpx"], "input_from": "subdomains+ports", "output_to": "live_hosts", "enabled": True},
                    {"name": "URL Discovery", "emoji": "ðŸ”—", "tools": ["gau", "waybackurls", "katana"], "input_from": "live_hosts", "output_to": "urls", "enabled": True},
                    {"name": "JS Analysis", "emoji": "ðŸ“œ", "tools": ["getjs"], "input_from": "live_hosts", "output_to": "js_files", "limit": 20, "enabled": True},
                    {"name": "Screenshots", "emoji": "ðŸ“¸", "tools": ["gowitness"], "input_from": "live_hosts", "output_to": "screenshots", "limit": 50, "enabled": True},
                    {"name": "Vulnerability Scan", "emoji": "ðŸŽ¯", "tools": ["nuclei"], "input_from": "live_hosts", "output_to": "vulnerabilities", "enabled": True},
                ]
            },
            "narrow": {
                "description": "Application-focused testing",
                "stages": [
                    {"name": "DNS Validation", "emoji": "ðŸ“¡", "tools": ["dnsx"], "input_from": "target", "output_to": "resolved", "enabled": True},
                    {"name": "Port Scan", "emoji": "ðŸ”Œ", "tools": ["naabu_quick"], "input_from": "target", "output_to": "ports", "enabled": True},
                    {"name": "HTTP Probing", "emoji": "ðŸŒ", "tools": ["httpx"], "input_from": "target", "output_to": "live_hosts", "enabled": True},
                    {"name": "Deep Crawling", "emoji": "ðŸ•·ï¸", "tools": ["katana_deep", "hakrawler"], "input_from": "live_hosts", "output_to": "urls", "enabled": True},
                    {"name": "URL Archives", "emoji": "ðŸ”—", "tools": ["gau", "waybackurls"], "input_from": "target", "output_to": "urls", "enabled": True},
                    {"name": "JS Analysis", "emoji": "ðŸ“œ", "tools": ["getjs"], "input_from": "live_hosts", "output_to": "js_files", "enabled": True},
                    {"name": "Content Discovery", "emoji": "ðŸ“‚", "tools": ["ffuf"], "input_from": "live_hosts", "output_to": "content", "limit": 3, "enabled": True},
                    {"name": "Screenshots", "emoji": "ðŸ“¸", "tools": ["gowitness"], "input_from": "live_hosts", "output_to": "screenshots", "enabled": True},
                    {"name": "Vulnerability Scan", "emoji": "ðŸŽ¯", "tools": ["nuclei"], "input_from": "live_hosts", "output_to": "vulnerabilities", "enabled": True},
                ]
            },
            "fast": {
                "description": "Quick scan for immediate wins",
                "stages": [
                    {"name": "Quick Subdomains", "emoji": "ðŸ”", "tools": ["subfinder", "crtsh"], "input_from": "target", "output_to": "subdomains", "enabled": True},
                    {"name": "HTTP Probing", "emoji": "ðŸŒ", "tools": ["httpx"], "input_from": "subdomains", "output_to": "live_hosts", "limit": 100, "enabled": True},
                    {"name": "Quick Vuln Scan", "emoji": "ðŸŽ¯", "tools": ["nuclei_fast"], "input_from": "live_hosts", "output_to": "vulnerabilities", "limit": 50, "enabled": True},
                ]
            }
        },
        "parsers": {
            "dnsx": {"host_field": "host", "data_field": "a"},
            "naabu": {"host_field": "host", "port_field": "port"},
            "httpx": {"url_field": "url", "tech_field": "tech", "status_field": "status_code", "title_field": "title"},
        },
        "discord": {"enabled": False, "webhook_url": "", "notify_on": ["scan_start", "scan_complete", "vulnerability_found"], "vuln_severity": ["critical", "high"]}
    }

def load_pipeline_config() -> Dict:
    """Load pipeline config from YAML file or use defaults"""
    config = get_default_pipeline_config()
    
    # Try loading from user config
    if PIPELINE_FILE.exists() and YAML_AVAILABLE:
        try:
            with open(PIPELINE_FILE) as f:
                user_config = yaml.safe_load(f)
                if user_config:
                    # Deep merge user config with defaults
                    config = deep_merge(config, user_config)
        except Exception as e:
            if console:
                console.print(f"[yellow]Warning: Could not load pipeline.yaml: {e}[/]")
    
    # Try bundled config
    elif BUNDLED_PIPELINE.exists() and YAML_AVAILABLE:
        try:
            with open(BUNDLED_PIPELINE) as f:
                bundled = yaml.safe_load(f)
                if bundled:
                    config = deep_merge(config, bundled)
        except:
            pass
    
    return config

def deep_merge(base: Dict, override: Dict) -> Dict:
    """Deep merge two dictionaries"""
    result = base.copy()
    for key, value in override.items():
        if key in result and isinstance(result[key], dict) and isinstance(value, dict):
            result[key] = deep_merge(result[key], value)
        else:
            result[key] = value
    return result

def save_pipeline_config(config: Dict):
    """Save pipeline config to YAML"""
    ensure_dir(CONFIG_DIR)
    if YAML_AVAILABLE:
        with open(PIPELINE_FILE, 'w') as f:
            yaml.dump(config, f, default_flow_style=False, sort_keys=False, allow_unicode=True)

# ============== ENUMS ==============

class Mode(Enum):
    WIDE = "wide"
    NARROW = "narrow"
    FAST = "fast"  # New: Quick scan with minimal tools

class Status(Enum):
    PENDING = "pending"
    RUNNING = "running"
    DONE = "done"
    FAILED = "failed"
    SKIPPED = "skipped"

# ============== THEME / COLORS ==============

class Theme:
    """Color theme for the UI"""
    PRIMARY = "cyan"
    SECONDARY = "blue"
    SUCCESS = "green"
    WARNING = "yellow"
    ERROR = "red"
    INFO = "white"
    DIM = "dim"
    HIGHLIGHT = "bold magenta"
    
    # Stage colors
    STAGE_SUBDOMAIN = "bold cyan"
    STAGE_DNS = "bold blue"
    STAGE_PORT = "bold yellow"
    STAGE_HTTP = "bold green"
    STAGE_URL = "bold magenta"
    STAGE_JS = "bold cyan"
    STAGE_SCREEN = "bold blue"
    STAGE_VULN = "bold red"

# ============== UTILITIES ==============

def ensure_dir(path: Path) -> Path:
    path.mkdir(parents=True, exist_ok=True)
    return path

def is_installed(tool: str) -> bool:
    return shutil.which(tool) is not None

def run_cmd(cmd: List[str], timeout: int = 300, use_proxy: bool = False, stdin_input: str = None) -> Tuple[int, str, str]:
    """Run command with optional proxychains"""
    if use_proxy and is_installed("proxychains4"):
        cmd = ["proxychains4", "-q"] + cmd
    try:
        result = subprocess.run(cmd, timeout=timeout, capture_output=True, text=True, input=stdin_input)
        return result.returncode, result.stdout, result.stderr
    except subprocess.TimeoutExpired:
        return -1, "", "Timeout"
    except Exception as e:
        return -1, "", str(e)

def sanitize(target: str) -> str:
    return re.sub(r'[^\w\-.]', '_', target.replace("https://", "").replace("http://", "").split("/")[0])

def strip_protocol(url: str) -> str:
    return url.replace("https://", "").replace("http://", "").split("/")[0].split(":")[0]

def save_list(filepath: Path, items):
    with open(filepath, 'w') as f:
        for item in sorted(set(items)):
            f.write(f"{item}\n")

def load_list(filepath: Path) -> List[str]:
    if filepath.exists():
        with open(filepath) as f:
            return [l.strip() for l in f if l.strip()]
    return []

def load_config() -> Dict:
    config_file = CONFIG_DIR / "config.json"
    default = {
        "discord": {"enabled": False, "webhook_url": "", "notify_on": ["scan_start", "scan_complete", "vulnerability"]},
        "proxy": {"enabled": True},
        "rate_limits": {"requests_per_second": 50, "threads": 25}
    }
    if config_file.exists():
        with open(config_file) as f:
            return {**default, **json.load(f)}
    return default

def save_config(config: Dict):
    ensure_dir(CONFIG_DIR)
    with open(CONFIG_DIR / "config.json", 'w') as f:
        json.dump(config, f, indent=2)

def get_timestamp() -> str:
    return datetime.now(timezone.utc).isoformat().replace('+00:00', 'Z')

# ============== SCAN CONTEXT ==============

@dataclass
class ScanContext:
    """Holds all scan data and state"""
    target: str
    mode: Mode
    output_dir: Path
    use_proxy: bool = True
    
    # Discovered assets
    subdomains: Set[str] = field(default_factory=set)
    resolved: Dict[str, List[str]] = field(default_factory=dict)
    live_hosts: Set[str] = field(default_factory=set)
    ports: Dict[str, List[int]] = field(default_factory=dict)
    urls: Set[str] = field(default_factory=set)
    js_files: Set[str] = field(default_factory=set)
    endpoints: Set[str] = field(default_factory=set)
    technologies: Dict[str, List[str]] = field(default_factory=dict)
    vulnerabilities: List[Dict] = field(default_factory=list)
    
    # Stats
    errors: List[str] = field(default_factory=list)
    start_time: datetime = field(default_factory=datetime.now)

# ============== TOOL RUNNER (YAML-DRIVEN) ==============

class ToolRunner:
    """Runs tools based on YAML pipeline configuration"""
    
    def __init__(self, pipeline_config: Dict, use_proxy: bool = True, rate_limit: int = 100, threads: int = 25):
        self.config = pipeline_config
        self.tools = pipeline_config.get("tools", {})
        self.parsers = pipeline_config.get("parsers", {})
        self.use_proxy = use_proxy and is_installed("proxychains4")
        self.rate_limit = rate_limit
        self.threads = threads
    
    def run_shell(self, cmd_str: str, timeout: int = 300, use_proxy: bool = None) -> Tuple[int, str, str]:
        """Execute a shell command string"""
        proxy = self.use_proxy if use_proxy is None else use_proxy
        if proxy and is_installed("proxychains4"):
            cmd_str = f"proxychains4 -q {cmd_str}"
        try:
            result = subprocess.run(cmd_str, shell=True, timeout=timeout, capture_output=True, text=True)
            return result.returncode, result.stdout, result.stderr
        except subprocess.TimeoutExpired:
            return -1, "", "Timeout"
        except Exception as e:
            return -1, "", str(e)
    
    def run_tool(self, tool_name: str, context: Dict) -> Tuple[bool, Any]:
        """
        Run a tool from the YAML config.
        context should contain: target, input_file, output_file, output_dir, threads, rate
        Returns: (success, result_data)
        """
        if tool_name not in self.tools:
            return False, f"Tool {tool_name} not defined in pipeline.yaml"
        
        tool_cfg = self.tools[tool_name]
        
        # Check if installed
        check_bin = tool_cfg.get("installed_check", tool_name)
        if not is_installed(check_bin):
            return False, f"{check_bin} not installed"
        
        # Build command from template
        cmd_template = tool_cfg.get("cmd", "")
        timeout = tool_cfg.get("timeout", 300)
        
        # Merge tool options with context
        options = tool_cfg.get("options", {})
        ctx = {**options, **context, "threads": self.threads, "rate": self.rate_limit}
        
        # Replace placeholders
        try:
            cmd = cmd_template.format(**ctx)
        except KeyError as e:
            return False, f"Missing placeholder: {e}"
        
        # Execute
        code, stdout, stderr = self.run_shell(cmd, timeout)
        
        if code != 0 and not stdout:
            return False, stderr or "Command failed"
        
        # Parse output based on type
        output_type = tool_cfg.get("output", "stdout_lines")
        
        if output_type == "stdout_lines":
            lines = [l.strip() for l in stdout.split('\n') if l.strip()]
            return True, set(lines)
        
        elif output_type == "json_lines":
            results = []
            parser_name = tool_cfg.get("parser")
            parser = self.parsers.get(parser_name, {}) if parser_name else {}
            
            for line in stdout.split('\n'):
                if line.strip():
                    try:
                        data = json.loads(line)
                        results.append(data)
                    except:
                        pass
            return True, results
        
        elif output_type == "file":
            output_file = context.get("output_file")
            if output_file and Path(output_file).exists():
                return True, Path(output_file)
            return True, None
        
        elif output_type == "directory":
            return True, context.get("output_dir")
        
        return True, stdout
    
    def parse_json_results(self, tool_name: str, results: List[Dict]) -> Any:
        """Parse JSON results based on tool parser config"""
        parser_name = self.tools.get(tool_name, {}).get("parser")
        if not parser_name:
            return results
        
        parser = self.parsers.get(parser_name, {})
        
        if parser_name == "dnsx":
            # Returns {host: [ips]}
            resolved = {}
            for r in results:
                host = r.get(parser.get("host_field", "host"), "")
                ips = r.get(parser.get("data_field", "a"), [])
                if host and ips:
                    resolved[host] = ips
            return resolved
        
        elif parser_name == "naabu":
            # Returns {host: [ports]}
            ports = {}
            for r in results:
                host = r.get(parser.get("host_field", "host"), "")
                port = r.get(parser.get("port_field", "port"), 0)
                if host and port:
                    ports.setdefault(host, []).append(port)
            return ports
        
        elif parser_name == "httpx":
            # Returns (live_urls, technologies)
            live = set()
            techs = {}
            for r in results:
                url = r.get(parser.get("url_field", "url"), "")
                if url:
                    live.add(url)
                    tech = r.get(parser.get("tech_field", "tech"), [])
                    if tech:
                        techs[url] = tech
            return (live, techs)
        
        return results

# ============== SCAN ENGINE (YAML-DRIVEN) ==============

class MacaronEngine:
    """Main scan orchestrator using YAML pipeline configuration"""
    
    def __init__(self, use_proxy: bool = True, rate: int = 100, threads: int = 25, quiet: bool = False, mode: str = "wide"):
        self.pipeline_config = load_pipeline_config()
        self.runner = ToolRunner(self.pipeline_config, use_proxy, rate, threads)
        self.quiet = quiet
        self.running = True
        self.notifier = DiscordNotifier()
        self.rate = rate
        self.threads = threads
        
        # Ensure directories
        for d in [DATA_DIR, CONFIG_DIR, STATE_DIR, LOGS_DIR, WORDLISTS_DIR]:
            ensure_dir(d)
        
        # Copy bundled pipeline.yaml to user config if not exists
        if not PIPELINE_FILE.exists() and BUNDLED_PIPELINE.exists():
            shutil.copy(BUNDLED_PIPELINE, PIPELINE_FILE)
        
        signal.signal(signal.SIGINT, self._handle_interrupt)
        signal.signal(signal.SIGTERM, self._handle_interrupt)
    
    def _handle_interrupt(self, signum, frame):
        self.running = False
        if console:
            console.print("\n[yellow]âš  Interrupt received, stopping gracefully...[/]")
    
    def scan(self, targets: List[str], mode: str = "wide") -> Dict:
        """Execute scan using YAML pipeline definition"""
        start = datetime.now()
        self.notifier.scan_start(targets, mode)
        
        stats = {
            "mode": mode,
            "targets": len(targets),
            "subdomains": 0,
            "live_hosts": 0,
            "ports": 0,
            "urls": 0,
            "vulnerabilities": 0,
            "errors": 0
        }
        
        if console and not self.quiet:
            console.print(BANNER.format(version=VERSION))
        
        # Get pipeline from config
        pipelines = self.pipeline_config.get("pipelines", {})
        if mode not in pipelines:
            if console:
                console.print(f"[red]Pipeline '{mode}' not found in pipeline.yaml[/]")
            return stats
        
        pipeline = pipelines[mode]
        
        for i, target in enumerate(targets):
            if not self.running:
                break
            
            # Context holds all discovered data
            ctx = {
                "target": target,
                "subdomains": set(),
                "resolved": {},
                "ports": {},
                "live_hosts": set(),
                "urls": set(),
                "js_files": set(),
                "vulnerabilities": [],
                "technologies": {},
                "errors": [],
                "output_dir": DATA_DIR / sanitize(target)
            }
            ensure_dir(ctx["output_dir"])
            
            if console and not self.quiet:
                console.print(f"\n[bold cyan]â”â”â” Target {i+1}/{len(targets)}: {target} â”â”â”[/]\n")
            
            # Run each stage in the pipeline
            for stage in pipeline.get("stages", []):
                if not self.running:
                    break
                if not stage.get("enabled", True):
                    continue
                
                ctx = self._run_pipeline_stage(stage, ctx)
            
            # Update stats
            stats["subdomains"] += len(ctx.get("subdomains", []))
            stats["live_hosts"] += len(ctx.get("live_hosts", []))
            stats["ports"] += sum(len(p) for p in ctx.get("ports", {}).values())
            stats["urls"] += len(ctx.get("urls", []))
            stats["vulnerabilities"] += len(ctx.get("vulnerabilities", []))
            stats["errors"] += len(ctx.get("errors", []))
            
            self._save_results(ctx)
        
        stats["duration"] = str(datetime.now() - start).split('.')[0]
        stats["status"] = "completed" if self.running else "interrupted"
        
        self.notifier.scan_complete(stats)
        self._print_summary(stats)
        
        return stats
    
    def _run_pipeline_stage(self, stage: Dict, ctx: Dict) -> Dict:
        """Run a single pipeline stage from YAML config"""
        name = stage.get("name", "Unknown")
        emoji = stage.get("emoji", "â–¶")
        tools = stage.get("tools", [])
        input_from = stage.get("input_from", "target")
        output_to = stage.get("output_to", "")
        limit = stage.get("limit", None)
        
        if not tools:
            return ctx
        
        # Determine input data
        if input_from == "target":
            input_data = [ctx["target"]]
        elif input_from == "subdomains":
            input_data = list(ctx.get("subdomains", set()))
        elif input_from == "resolved":
            input_data = list(ctx.get("resolved", {}).keys())
        elif input_from == "live_hosts":
            input_data = list(ctx.get("live_hosts", set()))
        elif input_from == "subdomains+ports":
            # Combine subdomains with non-standard ports
            input_data = list(ctx.get("subdomains", set()))
            for host, ports in ctx.get("ports", {}).items():
                for port in ports:
                    if port not in (80, 443):
                        input_data.append(f"{host}:{port}")
        elif input_from == "urls":
            input_data = list(ctx.get("urls", set()))
        else:
            input_data = [ctx["target"]]
        
        if not input_data:
            return ctx
        
        # Apply limit
        if limit and len(input_data) > limit:
            input_data = input_data[:limit]
        
        results = {}
        
        # Progress display
        if console and not self.quiet:
            with Progress(
                SpinnerColumn(),
                TextColumn(f"[bold cyan]{emoji} {name}[/]"),
                BarColumn(bar_width=30),
                TaskProgressColumn(),
                TextColumn("â€¢"),
                TimeElapsedColumn(),
                console=console,
                transient=True
            ) as progress:
                task = progress.add_task("", total=len(tools))
                
                for tool_name in tools:
                    if not self.running:
                        break
                    
                    progress.update(task, description=f"[dim]{tool_name}[/]")
                    result = self._execute_tool(tool_name, input_data, ctx)
                    if result is not None:
                        results[tool_name] = result
                    progress.advance(task)
        else:
            for tool_name in tools:
                if not self.running:
                    break
                result = self._execute_tool(tool_name, input_data, ctx)
                if result is not None:
                    results[tool_name] = result
        
        # Merge results into context based on output_to
        ctx = self._merge_results(results, output_to, ctx)
        
        # Save intermediate results
        self._save_stage_output(output_to, ctx)
        
        # Print stage completion
        count = self._get_count(output_to, ctx)
        if count > 0 and console and not self.quiet:
            color = "red" if output_to == "vulnerabilities" else "green"
            console.print(f"  [{color}]âœ“[/] {output_to}: [bold]{count}[/]")
        
        return ctx
    
    def _execute_tool(self, tool_name: str, input_data: List[str], ctx: Dict) -> Any:
        """Execute a single tool from YAML config"""
        tool_cfg = self.pipeline_config.get("tools", {}).get(tool_name, {})
        if not tool_cfg:
            return None
        
        input_type = tool_cfg.get("input", "target")
        
        # Create temp file if needed
        temp_file = None
        if input_type == "file":
            with tempfile.NamedTemporaryFile(mode='w', suffix='.txt', delete=False) as f:
                f.write('\n'.join(input_data))
                temp_file = f.name
        
        output_file = None
        if tool_cfg.get("output") in ("json_lines", "file"):
            output_file = str(ctx["output_dir"] / f"{tool_name}_output.json")
        
        # Build context for tool execution
        tool_ctx = {
            "target": input_data[0] if input_data else ctx["target"],
            "input_file": temp_file or "",
            "output_file": output_file or "",
            "output_dir": str(ctx["output_dir"] / tool_name),
            "wordlist": str(Path(tool_cfg.get("options", {}).get("wordlist", "~/.macaron/wordlists/common.txt")).expanduser()),
            "severity": tool_cfg.get("options", {}).get("severity", "critical,high,medium"),
            "depth": tool_cfg.get("options", {}).get("depth", 3),
        }
        
        try:
            success, result = self.runner.run_tool(tool_name, tool_ctx)
            
            if not success:
                ctx["errors"].append(f"{tool_name}: {result}")
                return None
            
            # Parse JSON results if needed
            if tool_cfg.get("output") == "json_lines" and isinstance(result, list):
                return self.runner.parse_json_results(tool_name, result)
            
            return result
            
        except Exception as e:
            ctx["errors"].append(f"{tool_name}: {e}")
            return None
        finally:
            if temp_file:
                Path(temp_file).unlink(missing_ok=True)
    
    def _merge_results(self, results: Dict, output_to: str, ctx: Dict) -> Dict:
        """Merge tool results into context"""
        for tool_name, result in results.items():
            if result is None:
                continue
            
            if output_to == "subdomains":
                if isinstance(result, (set, list)):
                    # Filter to only include subdomains of target
                    target = ctx["target"]
                    ctx["subdomains"].update(s.lower() for s in result if target in s.lower())
            
            elif output_to == "resolved":
                if isinstance(result, dict):
                    ctx["resolved"].update(result)
            
            elif output_to == "ports":
                if isinstance(result, dict):
                    for host, ports in result.items():
                        ctx["ports"].setdefault(host, []).extend(ports)
            
            elif output_to == "live_hosts":
                if isinstance(result, tuple):  # httpx returns (live, techs)
                    live, techs = result
                    ctx["live_hosts"].update(live)
                    ctx["technologies"].update(techs)
                elif isinstance(result, (set, list)):
                    ctx["live_hosts"].update(result)
            
            elif output_to == "urls":
                if isinstance(result, (set, list)):
                    ctx["urls"].update(result)
            
            elif output_to == "js_files":
                if isinstance(result, (set, list)):
                    ctx["js_files"].update(r for r in result if '.js' in r.lower())
            
            elif output_to == "vulnerabilities":
                if isinstance(result, list):
                    ctx["vulnerabilities"].extend(result)
                # Also read from nuclei output file
                vuln_file = ctx["output_dir"] / "nuclei.json"
                if vuln_file.exists():
                    with open(vuln_file) as f:
                        for line in f:
                            if line.strip():
                                try:
                                    v = json.loads(line)
                                    ctx["vulnerabilities"].append({
                                        "host": v.get("host", ""),
                                        "name": v.get("info", {}).get("name", ""),
                                        "severity": v.get("info", {}).get("severity", "info"),
                                        "template": v.get("template-id", ""),
                                    })
                                except:
                                    pass
        
        return ctx
    
    def _save_stage_output(self, output_to: str, ctx: Dict):
        """Save stage output to files"""
        output_dir = ctx["output_dir"]
        
        if output_to == "subdomains" and ctx.get("subdomains"):
            save_list(output_dir / "subdomains.txt", ctx["subdomains"])
        elif output_to == "resolved" and ctx.get("resolved"):
            save_list(output_dir / "resolved.txt", [f"{h}: {', '.join(map(str, ips))}" for h, ips in ctx["resolved"].items()])
        elif output_to == "ports" and ctx.get("ports"):
            save_list(output_dir / "ports.txt", [f"{h}:{p}" for h, ports in ctx["ports"].items() for p in ports])
        elif output_to == "live_hosts" and ctx.get("live_hosts"):
            save_list(output_dir / "live_hosts.txt", ctx["live_hosts"])
            if ctx.get("technologies"):
                save_list(output_dir / "technologies.txt", [f"{h}: {', '.join(t)}" for h, t in ctx["technologies"].items()])
        elif output_to == "urls" and ctx.get("urls"):
            save_list(output_dir / "urls.txt", ctx["urls"])
        elif output_to == "js_files" and ctx.get("js_files"):
            save_list(output_dir / "js_files.txt", ctx["js_files"])
    
    def _get_count(self, output_to: str, ctx: Dict) -> int:
        """Get count of items in output"""
        if output_to == "subdomains":
            return len(ctx.get("subdomains", []))
        elif output_to == "resolved":
            return len(ctx.get("resolved", {}))
        elif output_to == "ports":
            return sum(len(p) for p in ctx.get("ports", {}).values())
        elif output_to == "live_hosts":
            return len(ctx.get("live_hosts", []))
        elif output_to == "urls":
            return len(ctx.get("urls", []))
        elif output_to == "js_files":
            return len(ctx.get("js_files", []))
        elif output_to == "vulnerabilities":
            return len(ctx.get("vulnerabilities", []))
        elif output_to == "screenshots":
            ss_dir = ctx["output_dir"] / "gowitness"
            return len(list(ss_dir.glob("*.png"))) if ss_dir.exists() else 0
        return 0
    
    def _save_results(self, ctx: Dict):
        """Save final scan summary"""
        summary = {
            "target": ctx["target"],
            "scanned_at": get_timestamp(),
            "stats": {
                "subdomains": len(ctx.get("subdomains", [])),
                "resolved": len(ctx.get("resolved", {})),
                "live_hosts": len(ctx.get("live_hosts", [])),
                "ports": sum(len(p) for p in ctx.get("ports", {}).values()),
                "urls": len(ctx.get("urls", [])),
                "js_files": len(ctx.get("js_files", [])),
                "vulnerabilities": len(ctx.get("vulnerabilities", []))
            },
            "vulnerabilities": ctx.get("vulnerabilities", []),
            "errors": ctx.get("errors", [])
        }
        with open(ctx["output_dir"] / "summary.json", 'w') as f:
            json.dump(summary, f, indent=2)
    
    def _print_summary(self, stats: Dict):
        """Print scan summary"""
        if not console or self.quiet:
            return
        
        table = Table(title="Scan Results", box=box.ROUNDED, border_style="cyan")
        table.add_column("Metric", style="cyan")
        table.add_column("Value", style="bold white", justify="right")
        
        table.add_row("Mode", stats["mode"])
        table.add_row("Duration", stats["duration"])
        table.add_row("Subdomains", str(stats["subdomains"]))
        table.add_row("Live Hosts", str(stats["live_hosts"]))
        table.add_row("Open Ports", str(stats["ports"]))
        table.add_row("URLs", str(stats["urls"]))
        
        vuln_style = "bold red" if stats["vulnerabilities"] > 0 else "green"
        table.add_row("Vulnerabilities", f"[{vuln_style}]{stats['vulnerabilities']}[/]")
        
        console.print()
        console.print(table)
        console.print(f"\n[dim]Results: {DATA_DIR}[/]")
        console.print(f"[dim]Config:  {PIPELINE_FILE}[/]")

# ============== DISCORD NOTIFIER ==============

class DiscordNotifier:
    """Discord webhook notifications"""
    
    def __init__(self):
        config = load_config().get("discord", {})
        self.enabled = config.get("enabled", False)
        self.webhook_url = config.get("webhook_url", "")
    
    def _send(self, title: str, msg: str, color: int = 0x3498db):
        if not self.enabled or not self.webhook_url:
            return
        try:
            import urllib.request
            payload = json.dumps({
                "embeds": [{
                    "title": title,
                    "description": msg[:4000],
                    "color": color,
                    "footer": {"text": "Macaron v2"},
                    "timestamp": get_timestamp()
                }]
            }).encode()
            req = urllib.request.Request(self.webhook_url, data=payload, headers={"Content-Type": "application/json"})
            urllib.request.urlopen(req, timeout=10)
        except:
            pass
    
    def scan_start(self, targets: List[str], mode: str):
        self._send("ðŸš€ Scan Started", f"**Mode:** {mode}\n**Targets:** {', '.join(targets[:5])}", 0x3498db)
    
    def scan_complete(self, stats: Dict):
        self._send("âœ… Scan Complete", f"**Duration:** {stats.get('duration')}\n**Subdomains:** {stats.get('subdomains')}\n**Live:** {stats.get('live_hosts')}\n**Vulns:** {stats.get('vulnerabilities')}", 0x2ecc71)
    
    def vulnerability(self, name: str, severity: str, target: str):
        colors = {"critical": 0xff0000, "high": 0xff6600, "medium": 0xffff00}
        self._send(f"âš ï¸ {severity.upper()}: {name}", f"**Target:** {target}", colors.get(severity.lower(), 0x7289da))

# ============== CLI COMMANDS ==============

def cmd_scan(args):
    """Run a scan"""
    targets = []
    
    # File takes priority if specified
    if args.file:
        targets = load_list(Path(args.file))
    elif args.target:
        targets = args.target if isinstance(args.target, list) else [args.target]
    elif args.stdin:
        targets = [l.strip() for l in sys.stdin if l.strip()]
    
    if not targets:
        console.print("[red]âœ— No targets provided[/]") if console else print("No targets")
        console.print("[dim]Usage: macaron -s target.com OR macaron -F targets.txt[/]") if console else None
        return 1
    
    # Mode selection (now supports custom modes from pipeline.yaml)
    if args.mode:
        mode = args.mode
    elif args.fast:
        mode = "fast"
    elif args.narrow:
        mode = "narrow"
    else:
        mode = "wide"
    
    # Rate limiting
    rate = 10 if args.slow else (args.rate or 100)
    threads = 5 if args.slow else (args.threads or 25)
    
    engine = MacaronEngine(
        use_proxy=not args.no_proxy,
        rate=rate,
        threads=threads,
        quiet=args.quiet
    )
    
    stats = engine.scan(targets, mode=mode)
    return 0 if stats.get("status") == "completed" else 1

def cmd_status(args):
    """Show status"""
    if not console:
        print("Status requires rich library")
        return 1
    
    console.print(BANNER.format(version=VERSION))
    
    # Data summary
    if DATA_DIR.exists():
        table = Table(title="Scan Data", box=box.ROUNDED)
        table.add_column("Domain", style="cyan")
        table.add_column("Subdomains", justify="right")
        table.add_column("Live", justify="right")
        table.add_column("Vulns", justify="right", style="red")
        
        for d in sorted(DATA_DIR.iterdir())[:10]:
            if d.is_dir():
                subs = len(load_list(d / "subdomains.txt"))
                live = len(load_list(d / "live_hosts.txt"))
                vulns = 0
                vuln_file = d / "nuclei.json"
                if vuln_file.exists():
                    with open(vuln_file) as f:
                        vulns = len([l for l in f if l.strip()])
                table.add_row(d.name, str(subs), str(live), str(vulns))
        
        console.print(table)
    else:
        console.print("[dim]No scan data yet[/]")
    
    return 0

def cmd_show(args):
    """Show results"""
    if not console:
        return 1
    
    domain = args.domain
    what = args.what or "all"
    limit = args.limit or 50
    
    targets = []
    if DATA_DIR.exists():
        for d in DATA_DIR.iterdir():
            if d.is_dir() and (not domain or domain.lower() in d.name.lower()):
                targets.append(d)
    
    if not targets:
        console.print(f"[yellow]No results found[/]")
        return 1
    
    for target_dir in targets:
        console.print(f"\n[bold cyan]{'â•'*50}[/]")
        console.print(f"[bold cyan] {target_dir.name}[/]")
        console.print(f"[bold cyan]{'â•'*50}[/]")
        
        files = {
            "subdomains": ("subdomains.txt", "ðŸ”"),
            "live": ("live_hosts.txt", "ðŸŒ"),
            "ports": ("ports.txt", "ðŸ”Œ"),
            "urls": ("urls.txt", "ðŸ”—"),
            "js": ("js_files.txt", "ðŸ“œ"),
        }
        
        for key, (filename, emoji) in files.items():
            if what not in ["all", key]:
                continue
            filepath = target_dir / filename
            if filepath.exists():
                items = load_list(filepath)
                console.print(f"\n[bold]{emoji} {key.upper()}[/] ({len(items)})")
                for item in items[:limit]:
                    console.print(f"  [dim]{item}[/]")
                if len(items) > limit:
                    console.print(f"  [dim]... and {len(items) - limit} more[/]")
        
        # Vulnerabilities
        if what in ["all", "vulns"]:
            vuln_file = target_dir / "nuclei.json"
            if vuln_file.exists():
                vulns = []
                with open(vuln_file) as f:
                    for line in f:
                        if line.strip():
                            try:
                                vulns.append(json.loads(line))
                            except:
                                pass
                if vulns:
                    console.print(f"\n[bold red]ðŸŽ¯ VULNERABILITIES[/] ({len(vulns)})")
                    sev_colors = {"critical": "bold red", "high": "red", "medium": "yellow", "low": "green"}
                    for v in vulns[:limit]:
                        sev = v.get("info", {}).get("severity", "info")
                        name = v.get("info", {}).get("name", v.get("template-id", ""))
                        color = sev_colors.get(sev, "dim")
                        console.print(f"  [{color}][{sev.upper():8}][/] {name}")
    
    return 0

def cmd_tools(args):
    """List tools"""
    if not console:
        return 1
    
    categories = {
        "Subdomain": ["subfinder", "amass", "assetfinder", "findomain"],
        "DNS": ["dnsx", "massdns"],
        "Ports": ["naabu", "masscan", "nmap"],
        "HTTP": ["httpx", "httprobe"],
        "URLs": ["gau", "waybackurls", "katana", "hakrawler"],
        "JS": ["getJS"],
        "Content": ["ffuf", "feroxbuster"],
        "Vulns": ["nuclei"],
        "Screenshots": ["gowitness", "eyewitness"],
        "Utils": ["proxychains4", "jq", "curl"]
    }
    
    table = Table(title="Installed Tools", box=box.ROUNDED, border_style="cyan")
    table.add_column("Category", style="cyan")
    table.add_column("Tool")
    table.add_column("Status", justify="center")
    
    total, installed = 0, 0
    for cat, tools in categories.items():
        for tool in tools:
            total += 1
            if is_installed(tool):
                installed += 1
                table.add_row(cat, tool, "[green]âœ“[/]")
            else:
                table.add_row(cat, tool, "[red]âœ—[/]")
    
    console.print(table)
    pct = int((installed / total) * 100) if total else 0
    color = "green" if pct >= 70 else ("yellow" if pct >= 40 else "red")
    console.print(f"\n[{color}]{installed}/{total} ({pct}%) installed[/]")
    
    if installed < 10:
        console.print("\n[yellow]Tip:[/] Run 'sudo macaron -I' to install tools")
    
    return 0

def cmd_config(args):
    """Config management"""
    config = load_config()
    
    if args.webhook:
        config["discord"]["webhook_url"] = args.webhook
        config["discord"]["enabled"] = True
        save_config(config)
        if console:
            console.print("[green]âœ“ Discord webhook configured[/]")
        if args.test:
            DiscordNotifier().scan_start(["test.com"], "test")
            console.print("[green]âœ“ Test notification sent[/]") if console else None
    elif args.show:
        if console:
            console.print_json(json.dumps(config, indent=2))
        else:
            print(json.dumps(config, indent=2))
    
    return 0

def cmd_export(args):
    """Export results"""
    output = Path(args.output) if args.output else Path(f"macaron_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json")
    
    data = {"exported_at": get_timestamp(), "targets": {}}
    
    if DATA_DIR.exists():
        for d in DATA_DIR.iterdir():
            if d.is_dir():
                if args.domain and args.domain not in d.name:
                    continue
                target_data = {"name": d.name}
                for f in ["subdomains.txt", "live_hosts.txt", "urls.txt", "ports.txt"]:
                    fp = d / f
                    if fp.exists():
                        target_data[f.replace(".txt", "")] = load_list(fp)
                data["targets"][d.name] = target_data
    
    with open(output, 'w') as f:
        json.dump(data, f, indent=2)
    
    console.print(f"[green]âœ“ Exported to {output}[/]") if console else print(f"Exported to {output}")
    return 0

def cmd_install(args):
    """Install tools"""
    if os.geteuid() != 0:
        console.print("[red]âœ— Please run with sudo[/]") if console else print("Run with sudo")
        return 1
    
    console.print("[cyan]Installing reconnaissance tools...[/]") if console else print("Installing...")
    
    # Install Go tools
    go_tools = [
        "github.com/projectdiscovery/subfinder/v2/cmd/subfinder@latest",
        "github.com/projectdiscovery/httpx/cmd/httpx@latest",
        "github.com/projectdiscovery/dnsx/cmd/dnsx@latest",
        "github.com/projectdiscovery/naabu/v2/cmd/naabu@latest",
        "github.com/projectdiscovery/nuclei/v3/cmd/nuclei@latest",
        "github.com/projectdiscovery/katana/cmd/katana@latest",
        "github.com/tomnomnom/assetfinder@latest",
        "github.com/tomnomnom/waybackurls@latest",
        "github.com/sensepost/gowitness@latest",
        "github.com/ffuf/ffuf/v2@latest",
        "github.com/hakluke/hakrawler@latest",
        "github.com/lc/gau/v2/cmd/gau@latest",
        "github.com/003random/getJS@latest",
    ]
    
    os.environ["GOPATH"] = str(Path.home() / "go")
    os.environ["PATH"] = os.environ["PATH"] + ":" + str(Path.home() / "go" / "bin")
    
    for tool in go_tools:
        name = tool.split('/')[-1].split('@')[0]
        console.print(f"  Installing {name}...") if console else print(f"Installing {name}")
        subprocess.run(["go", "install", tool], capture_output=True)
    
    # Copy to /usr/local/bin
    go_bin = Path.home() / "go" / "bin"
    if go_bin.exists():
        for f in go_bin.iterdir():
            shutil.copy(str(f), "/usr/local/bin/")
    
    console.print("[green]âœ“ Installation complete![/]") if console else print("Done!")
    return 0

# ============== MAIN ==============

def main():
    parser = argparse.ArgumentParser(
        prog="macaron",
        description="Macaron v2.1 - YAML-Configured Security Reconnaissance",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  macaron -s example.com              Wide scan (default)
  macaron -s app.com -n               Narrow scan (app-focused)
  macaron -s target.com -f            Fast scan (quick wins)
  macaron -s target.com -m custom     Custom pipeline from YAML
  macaron -s target.com --slow        Slow mode (10 req/s)
  macaron -S                          Show status
  macaron -L                          List tools
  macaron -R -d example.com           Show results
  macaron -P                          Show pipeline config path

Pipeline Configuration:
  Edit ~/.macaron/config/pipeline.yaml to customize:
  - Tool commands and options
  - Scan stages and order
  - Rate limits and timeouts
  - Create custom scan modes
        """
    )
    
    # Main actions (short flags)
    parser.add_argument("-s", "--scan", dest="target", nargs="*", metavar="TARGET", help="Scan target(s)")
    parser.add_argument("-S", "--status", action="store_true", help="Show status & summary")
    parser.add_argument("-R", "--results", action="store_true", help="Show results")
    parser.add_argument("-L", "--list-tools", action="store_true", help="List installed tools")
    parser.add_argument("-E", "--export", action="store_true", help="Export results")
    parser.add_argument("-I", "--install", action="store_true", help="Install recon tools (sudo)")
    parser.add_argument("-C", "--config", action="store_true", help="Show config")
    parser.add_argument("-P", "--pipeline", action="store_true", help="Show pipeline.yaml path")
    
    # Scan options
    parser.add_argument("-m", "--mode", metavar="MODE", help="Scan mode (wide/narrow/fast/custom)")
    parser.add_argument("-f", "--fast", action="store_true", help="Fast mode (minimal tools)")
    parser.add_argument("-n", "--narrow", action="store_true", help="Narrow mode (app-focused)")
    parser.add_argument("-F", "--file", metavar="FILE", help="Targets from file")
    parser.add_argument("--stdin", action="store_true", help="Read from stdin")
    parser.add_argument("--no-proxy", action="store_true", help="Disable proxychains")
    parser.add_argument("--slow", action="store_true", help="Slow mode (10 req/s)")
    parser.add_argument("--rate", type=int, help="Rate limit (req/s)")
    parser.add_argument("--threads", type=int, help="Threads")
    parser.add_argument("-q", "--quiet", action="store_true", help="Quiet mode")
    
    # Results options
    parser.add_argument("-d", "--domain", help="Filter by domain")
    parser.add_argument("-w", "--what", choices=["all", "subdomains", "live", "ports", "urls", "js", "vulns"], help="What to show")
    parser.add_argument("--limit", type=int, default=50, help="Limit results")
    
    # Config options
    parser.add_argument("--webhook", metavar="URL", help="Set Discord webhook")
    parser.add_argument("--test", action="store_true", help="Test webhook")
    parser.add_argument("--show", action="store_true", help="Show config")
    
    # Export options
    parser.add_argument("-o", "--output", help="Output file")
    
    # Meta
    parser.add_argument("-v", "--verbose", action="store_true", help="Verbose")
    parser.add_argument("--version", action="version", version=f"macaron {VERSION}")
    
    args = parser.parse_args()
    
    # Route to appropriate command
    if args.target is not None or args.file or args.stdin:
        return cmd_scan(args)
    elif args.status:
        return cmd_status(args)
    elif args.results:
        return cmd_show(args)
    elif args.list_tools:
        return cmd_tools(args)
    elif args.pipeline:
        # Show pipeline config path
        ensure_dir(CONFIG_DIR)
        if not PIPELINE_FILE.exists() and BUNDLED_PIPELINE.exists():
            shutil.copy(BUNDLED_PIPELINE, PIPELINE_FILE)
        if console:
            console.print(f"\n[bold cyan]Pipeline Config:[/] {PIPELINE_FILE}")
            console.print(f"\n[dim]Edit this file to customize tool commands, stages, and modes.[/]")
            if PIPELINE_FILE.exists():
                console.print(f"\n[green]âœ“ Config exists[/]")
            else:
                console.print(f"\n[yellow]! Config will be created on first scan[/]")
        else:
            print(f"Pipeline config: {PIPELINE_FILE}")
        return 0
    elif args.export:
        return cmd_export(args)
    elif args.install:
        return cmd_install(args)
    elif args.config or args.webhook:
        return cmd_config(args)
    else:
        # Show help with banner
        if console:
            console.print(BANNER.format(version=VERSION))
        parser.print_help()
        return 0

if __name__ == "__main__":
    sys.exit(main())
