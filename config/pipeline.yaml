# ============================================================================
# MACARON v2.3 - Pipeline Configuration
# ============================================================================
# This file controls how scans are executed. Edit this to customize:
#   - Which tools run in each mode (wide/narrow/fast)
#   - Command-line arguments for each tool
#   - Timeouts, rate limits, and threading
#   - Pipeline order and data flow
#
# After editing, changes take effect on next scan. No restart needed.
# ============================================================================

# ------------------------------------------------------------------------------
# GLOBAL SETTINGS
# ------------------------------------------------------------------------------
global:
  # Default rate limit (requests per second)
  rate_limit: 100
  
  # Default threads for parallel operations
  threads: 25
  
  # Use proxychains4 by default (set to false to disable)
  use_proxy: true
  
  # Slow mode settings (--slow flag)
  slow_mode:
    rate_limit: 10
    threads: 5

# ------------------------------------------------------------------------------
# TOOL DEFINITIONS
# ------------------------------------------------------------------------------
# Each tool has:
#   - cmd: The command template. Use {placeholders} for dynamic values
#   - timeout: Max seconds to wait
#   - input: What this tool needs (target, file, stdin)
#   - output: What it produces (stdout_lines, json_lines, file)
#   - installed_check: Binary name to check if installed
#
# Available placeholders:
#   {target}     - The target domain
#   {input_file} - Temp file with input list
#   {output_file}- Output file path
#   {threads}    - Thread count from global/override
#   {rate}       - Rate limit from global/override
# ------------------------------------------------------------------------------

tools:
  # ============ SUBDOMAIN ENUMERATION ============
  
  subfinder:
    cmd: "subfinder -d {target} -silent -all -t {threads}"
    timeout: 600
    input: target
    output: stdout_lines
    installed_check: subfinder
    description: "Fast passive subdomain enumeration using multiple sources"
  
  amass:
    cmd: "amass enum -passive -d {target} -dns-qps 50 -timeout 10"
    timeout: 600
    input: target
    output: stdout_lines
    installed_check: amass
    description: "OWASP Amass passive enumeration (rate-limited)"
  
  assetfinder:
    cmd: "assetfinder --subs-only {target}"
    timeout: 300
    input: target
    output: stdout_lines
    installed_check: assetfinder
    description: "Quick asset/subdomain finder"
  
  findomain:
    cmd: "findomain -t {target} -q"
    timeout: 300
    input: target
    output: stdout_lines
    installed_check: findomain
    description: "Fast cross-platform subdomain finder"
  
  crtsh:
    cmd: "curl -s 'https://crt.sh/?q=%25.{target}&output=json' | jq -r '.[].name_value' | sort -u"
    timeout: 60
    input: target
    output: stdout_lines
    installed_check: curl
    description: "Certificate transparency log search"

  github_subdomains:
    cmd: "github-subdomains -d {target} -t {github_token} -o {output_file}"
    timeout: 300
    input: target
    output: file
    installed_check: github-subdomains
    description: "Find subdomains from GitHub source code"

  # ============ SUBDOMAIN PERMUTATION & BRUTEFORCE ============

  altdns:
    cmd: "altdns -i {input_file} -o {output_file} -w ~/.macaron/wordlists/altdns-words.txt"
    timeout: 600
    input: file
    output: file
    installed_check: altdns
    description: "Generate subdomain permutations"

  dnsgen:
    cmd: "cat {input_file} | dnsgen - | head -n 50000"
    timeout: 300
    input: file
    output: stdout_lines
    installed_check: dnsgen
    description: "Generate subdomain combinations from existing"

  shuffledns:
    cmd: "shuffledns -d {target} -w {wordlist} -r ~/.macaron/config/resolvers.txt -o {output_file} -silent"
    timeout: 1800
    input: target
    output: file
    installed_check: shuffledns
    options:
      wordlist: "~/.macaron/wordlists/subdomains.txt"
    description: "Subdomain bruteforce with massdns wrapper"

  puredns:
    cmd: "puredns bruteforce {wordlist} {target} -r ~/.macaron/config/resolvers.txt -q"
    timeout: 1800
    input: target
    output: stdout_lines
    installed_check: puredns
    options:
      wordlist: "~/.macaron/wordlists/subdomains.txt"
    description: "Fast subdomain bruteforce with wildcard detection"

  # ============ DNS RESOLUTION ============
  
  dnsx:
    cmd: "dnsx -l {input_file} -a -resp -json -silent -t {threads}"
    timeout: 600
    input: file
    output: json_lines
    installed_check: dnsx
    parser: dnsx
    description: "Fast DNS resolver with JSON output"

  dnsx_full:
    cmd: "dnsx -l {input_file} -a -aaaa -cname -mx -txt -resp -json -silent -t {threads}"
    timeout: 900
    input: file
    output: json_lines
    installed_check: dnsx
    parser: dnsx
    description: "Full DNS record enumeration"

  dnsrecon:
    cmd: "dnsrecon -d {target} -t axfr -j {output_file}"
    timeout: 120
    input: target
    output: file
    installed_check: dnsrecon
    description: "DNS zone transfer attempt"

  # ============ ASN & IP RANGE DISCOVERY ============

  asnmap:
    cmd: "echo {target} | asnmap -silent"
    timeout: 120
    input: target
    output: stdout_lines
    installed_check: asnmap
    description: "Map target to ASN and IP ranges"

  amass_intel:
    cmd: "amass intel -org '{target}' -max-dns-queries 100"
    timeout: 600
    input: target
    output: stdout_lines
    installed_check: amass
    description: "Discover ASNs and netblocks by organization"

  hakrevdns:
    cmd: "cat {input_file} | hakrevdns -t {threads}"
    timeout: 600
    input: file
    output: stdout_lines
    installed_check: hakrevdns
    description: "Reverse DNS lookup on IP ranges"

  mapcidr:
    cmd: "echo {target} | mapcidr -silent"
    timeout: 60
    input: target
    output: stdout_lines
    installed_check: mapcidr
    description: "CIDR range expansion"

  # ============ PORT SCANNING ============
  
  naabu:
    cmd: "naabu -l {input_file} -json -silent -top-ports 1000 -rate {rate} -c {threads} -retries 2"
    timeout: 1800
    input: file
    output: json_lines
    installed_check: naabu
    parser: naabu
    description: "Fast port scanner (top 1000 ports)"
  
  naabu_quick:
    cmd: "naabu -l {input_file} -json -silent -p 80,443,8080,8443,8000,3000,5000,9000 -rate {rate}"
    timeout: 300
    input: file
    output: json_lines
    installed_check: naabu
    parser: naabu
    description: "Quick port scan (common web ports only)"

  naabu_full:
    cmd: "naabu -l {input_file} -json -silent -p - -rate {rate} -c {threads}"
    timeout: 7200
    input: file
    output: json_lines
    installed_check: naabu
    parser: naabu
    description: "Full port scan (all 65535 ports)"

  # ============ HTTP PROBING ============
  
  httpx:
    cmd: "httpx -l {input_file} -json -silent -sc -title -td -cdn -follow-redirects -rate-limit {rate} -threads {threads}"
    timeout: 1800
    input: file
    output: json_lines
    installed_check: httpx
    parser: httpx
    description: "HTTP prober with tech detection and CDN identification"

  httpx_full:
    cmd: "httpx -l {input_file} -json -silent -sc -title -td -cdn -location -favicon -hash sha256 -jarm -follow-redirects -rate-limit {rate} -threads {threads}"
    timeout: 2400
    input: file
    output: json_lines
    installed_check: httpx
    parser: httpx
    description: "Full HTTP probe with favicon hash and JARM"

  # ============ WEB FINGERPRINTING ============

  whatweb:
    cmd: "whatweb -i {input_file} --log-json={output_file} -q -t {threads}"
    timeout: 1200
    input: file
    output: file
    installed_check: whatweb
    description: "Web technology fingerprinting"

  webanalyze:
    cmd: "webanalyze -hosts {input_file} -output json -silent"
    timeout: 600
    input: file
    output: stdout_lines
    installed_check: webanalyze
    description: "Wappalyzer-based tech detection"

  favfreak:
    cmd: "cat {input_file} | favfreak"
    timeout: 600
    input: file
    output: stdout_lines
    installed_check: favfreak
    description: "Favicon hash for asset correlation"

  # ============ URL DISCOVERY ============
  
  gau:
    cmd: "gau --subs --threads 5 {target}"
    timeout: 300
    input: target
    output: stdout_lines
    installed_check: gau
    description: "Fetch URLs from Wayback, Common Crawl, OTX, URLScan"
  
  waybackurls:
    cmd: "echo {target} | waybackurls"
    timeout: 300
    input: target
    output: stdout_lines
    installed_check: waybackurls
    description: "Fetch URLs from Wayback Machine"
  
  katana:
    cmd: "katana -list {input_file} -silent -d {depth} -jc -iqp -c {threads} -rl {rate}"
    timeout: 900
    input: file
    output: stdout_lines
    installed_check: katana
    options:
      depth: 3
    description: "Modern crawler with JavaScript parsing"
  
  katana_deep:
    cmd: "katana -list {input_file} -silent -d 5 -jc -iqp -c {threads} -rl {rate}"
    timeout: 1200
    input: file
    output: stdout_lines
    installed_check: katana
    description: "Deep crawling (depth 5)"
  
  hakrawler:
    cmd: "echo {target} | hakrawler -d 2 -subs -u"
    timeout: 300
    input: target
    output: stdout_lines
    installed_check: hakrawler
    description: "Fast web crawler for endpoints"

  gospider:
    cmd: "gospider -S {input_file} -o {output_dir} -c 10 -d 2 --other-source --include-subs -q"
    timeout: 900
    input: file
    output: directory
    installed_check: gospider
    description: "Fast web spider with multiple sources"

  # ============ PARAMETER DISCOVERY ============

  paramspider:
    cmd: "paramspider -d {target} -o {output_file} -s"
    timeout: 300
    input: target
    output: file
    installed_check: paramspider
    description: "Mine parameters from web archives"

  arjun:
    cmd: "arjun -i {input_file} -oJ {output_file} -t {threads} -q"
    timeout: 1200
    input: file
    output: file
    installed_check: arjun
    description: "HTTP parameter discovery"

  x8:
    cmd: "x8 -u {target} -w ~/.macaron/wordlists/params.txt -o {output_file}"
    timeout: 600
    input: target
    output: file
    installed_check: x8
    description: "Hidden parameter discovery"

  # ============ API DISCOVERY ============

  kiterunner:
    cmd: "kr scan {input_file} -w ~/.macaron/wordlists/routes.kite -o json -q"
    timeout: 1800
    input: file
    output: stdout_lines
    installed_check: kr
    description: "API endpoint discovery"

  # ============ JS ANALYSIS ============
  
  getjs:
    cmd: "getJS --url {target} --complete"
    timeout: 120
    input: target
    output: stdout_lines
    installed_check: getJS
    description: "Extract JavaScript file URLs"

  subjs:
    cmd: "cat {input_file} | subjs"
    timeout: 300
    input: file
    output: stdout_lines
    installed_check: subjs
    description: "Extract JS files from list of URLs"

  linkfinder:
    cmd: "linkfinder -i {target} -o cli"
    timeout: 120
    input: target
    output: stdout_lines
    installed_check: linkfinder
    description: "Find endpoints in JS files"

  # ============ CONTENT DISCOVERY ============
  
  ffuf:
    cmd: "ffuf -u {target}/FUZZ -w {wordlist} -o {output_file} -of json -mc 200,201,204,301,302,307,401,403,405 -t {threads} -rate {rate} -s"
    timeout: 600
    input: target
    output: file
    installed_check: ffuf
    options:
      wordlist: "~/.macaron/wordlists/common.txt"
    description: "Fast web fuzzer for content discovery"

  feroxbuster:
    cmd: "feroxbuster -u {target} -w {wordlist} -o {output_file} -q -t {threads}"
    timeout: 1200
    input: target
    output: file
    installed_check: feroxbuster
    options:
      wordlist: "~/.macaron/wordlists/common.txt"
    description: "Recursive content discovery"

  # ============ CLOUD ENUMERATION ============

  cloud_enum:
    cmd: "cloud_enum -k {target} -l {output_file} --disable-azure --disable-gcp"
    timeout: 600
    input: target
    output: file
    installed_check: cloud_enum
    description: "Enumerate cloud resources (S3, Azure, GCP)"

  s3scanner:
    cmd: "s3scanner scan -b {input_file} -o {output_file}"
    timeout: 600
    input: file
    output: file
    installed_check: s3scanner
    description: "Scan for open S3 buckets"

  # ============ SUBDOMAIN TAKEOVER ============

  subjack:
    cmd: "subjack -w {input_file} -t {threads} -o {output_file} -ssl -v"
    timeout: 600
    input: file
    output: file
    installed_check: subjack
    description: "Subdomain takeover detection"

  nuclei_takeover:
    cmd: "nuclei -l {input_file} -t ~/nuclei-templates/takeovers/ -o {output_file} -silent"
    timeout: 900
    input: file
    output: file
    installed_check: nuclei
    description: "Subdomain takeover via nuclei templates"

  # ============ OSINT ============

  theHarvester:
    cmd: "theHarvester -d {target} -b all -f {output_file}"
    timeout: 600
    input: target
    output: file
    installed_check: theHarvester
    description: "Email and subdomain OSINT harvesting"

  emailfinder:
    cmd: "emailfinder -d {target}"
    timeout: 300
    input: target
    output: stdout_lines
    installed_check: emailfinder
    description: "Find email addresses for domain"

  # ============ SHODAN & CENSYS ============

  shodan_domain:
    cmd: "shodan domain {target}"
    timeout: 120
    input: target
    output: stdout_lines
    installed_check: shodan
    description: "Shodan domain lookup"

  shodan_search:
    cmd: "shodan search 'hostname:{target}' --fields ip_str,port,org,hostnames"
    timeout: 120
    input: target
    output: stdout_lines
    installed_check: shodan
    description: "Shodan host search"

  # ============ SCREENSHOTS ============
  
  gowitness:
    cmd: "gowitness scan file -f {input_file} --screenshot-path {output_dir} --delay 2 --threads {threads}"
    timeout: 3600
    input: file
    output: directory
    installed_check: gowitness
    description: "Screenshot web pages (gowitness v3)"

  # ============ ROBOTS & SITEMAP ============

  robots_parser:
    cmd: "cat {input_file} | xargs -I {} curl -s {}/robots.txt | grep -E '^(Allow|Disallow):' | awk '{print $2}'"
    timeout: 300
    input: file
    output: stdout_lines
    installed_check: curl
    description: "Parse robots.txt for paths"

  sitemap_parser:
    cmd: "cat {input_file} | xargs -I {} curl -s {}/sitemap.xml | grep -oP '(?<=<loc>)[^<]+'"
    timeout: 300
    input: file
    output: stdout_lines
    installed_check: curl
    description: "Parse sitemap.xml for URLs"

  # ============ VULNERABILITY SCANNING (Removed - pure recon only) ============
  # Note: nuclei kept only for subdomain takeover detection

# ------------------------------------------------------------------------------
# SCAN PIPELINES
# ------------------------------------------------------------------------------
pipelines:
  # ============ WIDE MODE (Full Infrastructure) ============
  wide:
    description: "Full infrastructure reconnaissance"
    stages:
      - name: "Subdomain Discovery"
        emoji: "ðŸ”"
        tools: [subfinder, assetfinder, findomain, crtsh, amass]
        input_from: target
        output_to: subdomains
        enabled: true
      
      - name: "Subdomain Permutation"
        emoji: "ðŸ”€"
        tools: [dnsgen]
        input_from: subdomains
        output_to: subdomains
        enabled: true

      - name: "DNS Resolution"
        emoji: "ðŸ“¡"
        tools: [dnsx]
        input_from: subdomains
        output_to: resolved
        enabled: true
      
      - name: "Port Scanning"
        emoji: "ðŸ”Œ"
        tools: [naabu]
        input_from: resolved
        output_to: ports
        enabled: true
      
      - name: "HTTP Probing"
        emoji: "ðŸŒ"
        tools: [httpx]
        input_from: subdomains+ports
        output_to: live_hosts
        enabled: true
      
      - name: "Web Fingerprinting"
        emoji: "ðŸ”¬"
        tools: [whatweb]
        input_from: live_hosts
        output_to: technologies
        limit: 100
        enabled: true

      - name: "URL Discovery"
        emoji: "ðŸ”—"
        tools: [gau, waybackurls, katana]
        input_from: live_hosts
        output_to: urls
        enabled: true

      - name: "Robots & Sitemap"
        emoji: "ðŸ¤–"
        tools: [robots_parser, sitemap_parser]
        input_from: live_hosts
        output_to: urls
        enabled: true

      - name: "Parameter Mining"
        emoji: "âš™ï¸"
        tools: [paramspider]
        input_from: target
        output_to: parameters
        enabled: true
      
      - name: "JS Extraction"
        emoji: "ðŸ“œ"
        tools: [getjs, subjs]
        input_from: live_hosts
        output_to: js_files
        limit: 50
        enabled: true
      
      - name: "Screenshots"
        emoji: "ðŸ“¸"
        tools: [gowitness]
        input_from: live_hosts
        output_to: screenshots
        limit: 100
        enabled: true

      - name: "Subdomain Takeover"
        emoji: "ðŸš¨"
        tools: [subjack]
        input_from: subdomains
        output_to: takeovers
        enabled: true

  # ============ NARROW MODE (Application Focused) ============
  narrow:
    description: "Application-focused testing"
    stages:
      - name: "DNS Validation"
        emoji: "ðŸ“¡"
        tools: [dnsx_full]
        input_from: target
        output_to: resolved
        enabled: true
      
      - name: "Port Scan"
        emoji: "ðŸ”Œ"
        tools: [naabu_quick]
        input_from: target
        output_to: ports
        enabled: true
      
      - name: "HTTP Probing"
        emoji: "ðŸŒ"
        tools: [httpx_full]
        input_from: target
        output_to: live_hosts
        enabled: true

      - name: "Web Fingerprinting"
        emoji: "ðŸ”¬"
        tools: [whatweb]
        input_from: live_hosts
        output_to: technologies
        enabled: true
      
      - name: "Deep Crawling"
        emoji: "ðŸ•·ï¸"
        tools: [katana_deep, hakrawler, gospider]
        input_from: live_hosts
        output_to: urls
        enabled: true
      
      - name: "URL Archives"
        emoji: "ðŸ”—"
        tools: [gau, waybackurls]
        input_from: target
        output_to: urls
        enabled: true

      - name: "Robots & Sitemap"
        emoji: "ðŸ¤–"
        tools: [robots_parser, sitemap_parser]
        input_from: live_hosts
        output_to: urls
        enabled: true

      - name: "Parameter Discovery"
        emoji: "âš™ï¸"
        tools: [paramspider, arjun]
        input_from: live_hosts
        output_to: parameters
        limit: 10
        enabled: true
      
      - name: "JS Analysis"
        emoji: "ðŸ“œ"
        tools: [getjs, linkfinder]
        input_from: live_hosts
        output_to: js_files
        enabled: true
      
      - name: "Content Discovery"
        emoji: "ðŸ“‚"
        tools: [ffuf]
        input_from: live_hosts
        output_to: content
        limit: 5
        enabled: true
      
      - name: "Screenshots"
        emoji: "ðŸ“¸"
        tools: [gowitness]
        input_from: live_hosts
        output_to: screenshots
        enabled: true

  # ============ FAST MODE (Quick Recon) ============
  fast:
    description: "Quick reconnaissance scan"
    stages:
      - name: "Quick Subdomains"
        emoji: "ðŸ”"
        tools: [subfinder, crtsh]
        input_from: target
        output_to: subdomains
        enabled: true
      
      - name: "HTTP Probing"
        emoji: "ðŸŒ"
        tools: [httpx]
        input_from: subdomains
        output_to: live_hosts
        limit: 100
        enabled: true
      
      - name: "Quick URLs"
        emoji: "ðŸ”—"
        tools: [gau]
        input_from: target
        output_to: urls
        enabled: true

      - name: "Screenshots"
        emoji: "ðŸ“¸"
        tools: [gowitness]
        input_from: live_hosts
        output_to: screenshots
        limit: 50
        enabled: true

  # ============ OSINT MODE (Intelligence Gathering) ============
  osint:
    description: "OSINT and passive reconnaissance"
    stages:
      - name: "Subdomain Discovery"
        emoji: "ðŸ”"
        tools: [subfinder, amass, crtsh]
        input_from: target
        output_to: subdomains
        enabled: true

      - name: "ASN Discovery"
        emoji: "ðŸŒ"
        tools: [asnmap, amass_intel]
        input_from: target
        output_to: asn_info
        enabled: true

      - name: "Email Harvesting"
        emoji: "ðŸ“§"
        tools: [theHarvester]
        input_from: target
        output_to: emails
        enabled: true

      - name: "Shodan Recon"
        emoji: "ðŸ‘ï¸"
        tools: [shodan_domain]
        input_from: target
        output_to: shodan_data
        enabled: true

      - name: "Cloud Enumeration"
        emoji: "â˜ï¸"
        tools: [cloud_enum]
        input_from: target
        output_to: cloud_assets
        enabled: true

  # ============ DEEP MODE (Comprehensive) ============
  deep:
    description: "Deep comprehensive reconnaissance"
    stages:
      - name: "Subdomain Discovery"
        emoji: "ðŸ”"
        tools: [subfinder, assetfinder, findomain, crtsh, amass]
        input_from: target
        output_to: subdomains
        enabled: true

      - name: "Subdomain Bruteforce"
        emoji: "ðŸ”¨"
        tools: [shuffledns]
        input_from: target
        output_to: subdomains
        enabled: true

      - name: "Subdomain Permutation"
        emoji: "ðŸ”€"
        tools: [dnsgen, altdns]
        input_from: subdomains
        output_to: subdomains
        enabled: true

      - name: "DNS Resolution"
        emoji: "ðŸ“¡"
        tools: [dnsx_full]
        input_from: subdomains
        output_to: resolved
        enabled: true

      - name: "Reverse DNS"
        emoji: "ðŸ”„"
        tools: [hakrevdns]
        input_from: resolved
        output_to: reverse_dns
        enabled: true

      - name: "Full Port Scan"
        emoji: "ðŸ”Œ"
        tools: [naabu]
        input_from: resolved
        output_to: ports
        enabled: true

      - name: "HTTP Probing"
        emoji: "ðŸŒ"
        tools: [httpx_full]
        input_from: subdomains+ports
        output_to: live_hosts
        enabled: true

      - name: "Favicon Hashing"
        emoji: "ðŸŽ¨"
        tools: [favfreak]
        input_from: live_hosts
        output_to: favicon_hashes
        enabled: true

      - name: "Web Fingerprinting"
        emoji: "ðŸ”¬"
        tools: [whatweb, webanalyze]
        input_from: live_hosts
        output_to: technologies
        enabled: true

      - name: "Deep Crawling"
        emoji: "ðŸ•·ï¸"
        tools: [katana_deep, gospider]
        input_from: live_hosts
        output_to: urls
        enabled: true

      - name: "URL Archives"
        emoji: "ðŸ”—"
        tools: [gau, waybackurls]
        input_from: target
        output_to: urls
        enabled: true

      - name: "Robots & Sitemap"
        emoji: "ðŸ¤–"
        tools: [robots_parser, sitemap_parser]
        input_from: live_hosts
        output_to: urls
        enabled: true

      - name: "Parameter Discovery"
        emoji: "âš™ï¸"
        tools: [paramspider, arjun]
        input_from: live_hosts
        output_to: parameters
        limit: 20
        enabled: true

      - name: "API Discovery"
        emoji: "ðŸ”Œ"
        tools: [kiterunner]
        input_from: live_hosts
        output_to: api_endpoints
        limit: 20
        enabled: true

      - name: "JS Extraction"
        emoji: "ðŸ“œ"
        tools: [getjs, subjs, linkfinder]
        input_from: live_hosts
        output_to: js_files
        enabled: true

      - name: "Content Discovery"
        emoji: "ðŸ“‚"
        tools: [ffuf, feroxbuster]
        input_from: live_hosts
        output_to: content
        limit: 10
        enabled: true

      - name: "Cloud Enumeration"
        emoji: "â˜ï¸"
        tools: [cloud_enum, s3scanner]
        input_from: target
        output_to: cloud_assets
        enabled: true

      - name: "Subdomain Takeover"
        emoji: "ðŸš¨"
        tools: [subjack, nuclei_takeover]
        input_from: subdomains
        output_to: takeovers
        enabled: true

      - name: "Screenshots"
        emoji: "ðŸ“¸"
        tools: [gowitness]
        input_from: live_hosts
        output_to: screenshots
        enabled: true

# ------------------------------------------------------------------------------
# OUTPUT PARSERS
# ------------------------------------------------------------------------------
parsers:
  dnsx:
    host_field: "host"
    data_field: "a"
    
  naabu:
    host_field: "host"
    port_field: "port"
    
  httpx:
    url_field: "url"
    tech_field: "tech"
    status_field: "status_code"
    title_field: "title"
    favicon_field: "favicon_hash"

# ------------------------------------------------------------------------------
# NOTIFICATIONS
# ------------------------------------------------------------------------------
discord:
  enabled: false
  webhook_url: ""
  notify_on:
    - scan_start
    - scan_complete
    - new_assets
  
  vuln_severity: [critical, high]
